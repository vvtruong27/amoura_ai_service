{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.345401Z",
     "start_time": "2025-06-09T05:02:05.429617Z"
    }
   },
   "source": [
    "import joblib\n",
    "# Standard libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical computations\n",
    "import re            # For regular expressions (text cleaning)\n",
    "\n",
    "# Scikit-learn tools\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder  # For scaling and encoding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    # For text vectorization (interests, bio, etc.)\n",
    "from sklearn.metrics.pairwise import cosine_similarity         # For similarity measurement between text vectors\n",
    "\n",
    "# Natural Language Processing (NLP) - NLTK\n",
    "from nltk.corpus import stopwords                              # To remove common stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer         # For stemming and lemmatization\n",
    "from nltk.tokenize import word_tokenize                        # Tokenizer for breaking text into words\n",
    "\n",
    "# Text normalization\n",
    "from unidecode import unidecode                                # To remove accents from text (e.g., café → cafe)\n",
    "\n",
    "# Geographic distance\n",
    "from geopy.distance import geodesic                            # To compute distance between two geographic coordinates\n",
    "\n",
    "# Counter utility for frequency counting\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')       # Needed for word_tokenize\n",
    "nltk.download('stopwords')   # Needed to remove common stopwords\n",
    "nltk.download('wordnet')     # Needed for lemmatization"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/truongvu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/truongvu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/truongvu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.464820Z",
     "start_time": "2025-06-09T05:02:06.421721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Phase 2: Feature Engineering ---\n",
    "print(\"--- Phase 2: Feature Engineering ---\")\n",
    "print(\"\\n--- 2.0 Loading Processed Data from Phase 1 ---\")\n",
    "\n",
    "try:\n",
    "    # Load the profile and training pair datasets saved from Phase 1\n",
    "    profiles_df = pd.read_csv(\"../data/profiles_processed_phase1.csv\")\n",
    "    training_data_df = pd.read_csv(\"../data/training_pairs_phase1.csv\")\n",
    "    print(\"Successfully loaded preprocessed data from Phase 1!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the expected files are missing\n",
    "    print(\"Error: Could not find 'profiles_processed_phase1.csv' or 'training_pairs_phase1.csv'.\")\n",
    "    print(\"Make sure Phase 1 has been executed and the output files were saved correctly.\")"
   ],
   "id": "85f6b941a99ca43d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 2: Feature Engineering ---\n",
      "\n",
      "--- 2.0 Loading Processed Data from Phase 1 ---\n",
      "Successfully loaded preprocessed data from Phase 1!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.487374Z",
     "start_time": "2025-06-09T05:02:06.479452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2.1 Create User Features ---\n",
    "print(\"\\n--- 2.1 Creating User Features ---\")\n",
    "user_features_df = profiles_df.copy()\n",
    "\n",
    "# --- 2.1.1 Handle and Normalize Numerical Features ---\n",
    "print(\"\\n--- Handling numerical features ---\")\n",
    "\n",
    "# Assume 'age' was calculated in Phase 1\n",
    "# Now handle 'height' and normalize both 'age' and 'height'\n",
    "\n",
    "numerical_cols = ['age', 'height']\n",
    "for col in numerical_cols:\n",
    "    if user_features_df[col].isnull().any():\n",
    "        median_val = user_features_df[col].median()\n",
    "        user_features_df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "# Normalize 'age' using MinMaxScaler\n",
    "scaler_age = MinMaxScaler()\n",
    "user_features_df['age_scaled'] = scaler_age.fit_transform(user_features_df[['age']])\n",
    "\n",
    "joblib.dump(scaler_age, \"../models/scaler_age.joblib\")\n",
    "\n",
    "# Normalize 'height' using MinMaxScaler\n",
    "scaler_height = MinMaxScaler()\n",
    "user_features_df['height_scaled'] = scaler_height.fit_transform(user_features_df[['height']])\n",
    "joblib.dump(scaler_height, \"../models/scaler_height.joblib\")\n",
    "\n",
    "print(\"Successfully normalized 'age' and 'height'.\")"
   ],
   "id": "b4f663e3da3dc140",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.1 Creating User Features ---\n",
      "\n",
      "--- Handling numerical features ---\n",
      "Successfully normalized 'age' and 'height'.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.520464Z",
     "start_time": "2025-06-09T05:02:06.507926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2.1.2 Encode Categorical Features ---\n",
    "print(\"\\n--- Encoding categorical features ---\")\n",
    "\n",
    "# Define columns to encode using One-Hot Encoding\n",
    "categorical_cols_onehot = ['sex', 'orientation', 'body_type', 'drink', 'smoke']\n",
    "\n",
    "# Fill missing values in categorical columns with the mode (most frequent value)\n",
    "for col in categorical_cols_onehot:\n",
    "    if user_features_df[col].isnull().any():\n",
    "        mode_val = user_features_df[col].mode()[0]\n",
    "        user_features_df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with mode: {mode_val}\")\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform categorical columns\n",
    "encoded_cols = onehot_encoder.fit_transform(user_features_df[categorical_cols_onehot])\n",
    "\n",
    "# Convert encoded numpy array into a DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_cols,\n",
    "    columns=onehot_encoder.get_feature_names_out(categorical_cols_onehot),\n",
    "    index=user_features_df.index  # Ensure index aligns with original DataFrame\n",
    ")\n",
    "\n",
    "# Combine encoded columns with original DataFrame\n",
    "user_features_df = pd.concat([user_features_df.drop(columns=categorical_cols_onehot), encoded_df], axis=1)\n",
    "joblib.dump(onehot_encoder, \"../models/onehot_encoder_categorical.joblib\")\n",
    "\n",
    "print(f\"One-Hot Encoded the following categorical columns: {categorical_cols_onehot}\")"
   ],
   "id": "cfef06b977262462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoding categorical features ---\n",
      "One-Hot Encoded the following categorical columns: ['sex', 'orientation', 'body_type', 'drink', 'smoke']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.554763Z",
     "start_time": "2025-06-09T05:02:06.540601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Encode high-cardinality categorical columns using Top-N One-Hot Encoding ---\n",
    "print(\"\\n--- Encoding high-cardinality categorical columns ---\")\n",
    "\n",
    "# Utility function: Encode Top-N frequent categories, others grouped into 'other'\n",
    "def encode_top_n_categorical(df, column, top_n=10, prefix=None):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column using one-hot encoding for the top N most frequent values.\n",
    "    Remaining categories are grouped under a column '<prefix>_other'.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        column (str): Column to encode\n",
    "        top_n (int): Number of top frequent categories to keep\n",
    "        prefix (str): Prefix for the new columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Updated DataFrame with encoded columns\n",
    "    \"\"\"\n",
    "    # Count frequency of each category\n",
    "    counts = df[column].value_counts()\n",
    "\n",
    "    # Select Top-N categories\n",
    "    top_categories = counts.nlargest(top_n).index\n",
    "\n",
    "    # Encode each top category into a binary column\n",
    "    for category in top_categories:\n",
    "        col_name = f\"{prefix or column}_{unidecode(str(category)).lower().replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '').replace('.', '')}\"\n",
    "        df[col_name] = (df[column] == category).astype(int)\n",
    "\n",
    "    # Add binary column for all 'other' categories\n",
    "    df[f\"{prefix or column}_other\"] = (~df[column].isin(top_categories)).astype(int)\n",
    "\n",
    "    # Drop original column\n",
    "    return df.drop(columns=[column])\n",
    "\n",
    "# --- Encode 'job' column ---\n",
    "if 'job' in user_features_df.columns:\n",
    "    user_features_df['job'] = user_features_df['job'].fillna('unknown')\n",
    "    # Lấy top categories cho job\n",
    "    job_counts = user_features_df['job'].value_counts()\n",
    "    top_n_job_actual = 15 # Giống top_n đã dùng\n",
    "    top_job_categories_list = job_counts.nlargest(top_n_job_actual).index.tolist()\n",
    "    joblib.dump(top_job_categories_list, \"../models/top_n_job_categories.joblib\")\n",
    "    print(f\"Saved top_n_job_categories.joblib (Top {len(top_job_categories_list)} categories)\")\n",
    "\n",
    "    user_features_df = encode_top_n_categorical(user_features_df, 'job', top_n=top_n_job_actual, prefix='job')\n",
    "    print(\"Encoded Top-N categories for 'job'.\")\n",
    "\n",
    "# --- Encode 'education_level' column ---\n",
    "if 'education_level' in user_features_df.columns:\n",
    "    user_features_df['education_level'] = user_features_df['education_level'].fillna('unknown')\n",
    "    # Lấy top categories cho education_level\n",
    "    edu_counts = user_features_df['education_level'].value_counts()\n",
    "    top_n_edu_actual = 7 # Giống top_n đã dùng\n",
    "    top_edu_categories_list = edu_counts.nlargest(top_n_edu_actual).index.tolist()\n",
    "    joblib.dump(top_edu_categories_list, \"../models/top_n_edu_categories.joblib\")\n",
    "    print(f\"Saved top_n_edu_categories.joblib (Top {len(top_edu_categories_list)} categories)\")\n",
    "\n",
    "    user_features_df = encode_top_n_categorical(user_features_df, 'education_level', top_n=top_n_edu_actual, prefix='edu')\n",
    "    print(\"Encoded Top-N categories for 'education_level'.\")"
   ],
   "id": "40cbf065a2a233c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoding high-cardinality categorical columns ---\n",
      "Saved top_n_job_categories.joblib (Top 14 categories)\n",
      "Encoded Top-N categories for 'job'.\n",
      "Saved top_n_edu_categories.joblib (Top 6 categories)\n",
      "Encoded Top-N categories for 'education_level'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.576731Z",
     "start_time": "2025-06-09T05:02:06.573520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Handle 'dropped_out_school' as binary indicator ---\n",
    "if 'dropped_out_school' in user_features_df.columns:\n",
    "    user_features_df['dropped_out_school'] = user_features_df['dropped_out_school'].fillna(0).astype(int)\n",
    "    print(\"Handled missing values in 'dropped_out_school'.\")\n",
    "\n",
    "# --- Handle 'interested_in_new_language' as binary indicator ---\n",
    "if 'interested_in_new_language' in user_features_df.columns:\n",
    "    user_features_df['interested_in_new_language'] = user_features_df['interested_in_new_language'].fillna(0).astype(int)\n",
    "    print(\"Handled missing values in 'interested_in_new_language'.\")"
   ],
   "id": "941cc5cb994da4ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handled missing values in 'dropped_out_school'.\n",
      "Handled missing values in 'interested_in_new_language'.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.605954Z",
     "start_time": "2025-06-09T05:02:06.601161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load English stopwords (you can extend this list manually for Vietnamese later)\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# --- Text preprocessing function ---\n",
    "def preprocess_text(text, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses raw text input.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Raw input text\n",
    "        use_stemming (bool): Whether to apply stemming\n",
    "        use_lemmatization (bool): Whether to apply lemmatization\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize text: lowercase, remove accents\n",
    "    text = unidecode(str(text).lower())\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and very short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words_en and len(word) > 1]\n",
    "\n",
    "    # Apply lemmatization or stemming\n",
    "    if use_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    elif use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def create_multivalue_binary_features(df, column, separator=',', top_n=20, prefix=None, drop_original=False):\n",
    "    \"\"\"\n",
    "    Converts a multi-valued column into multiple binary columns based on the top-N most frequent items.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        column (str): Name of the column to process\n",
    "        separator (str): Delimiter used to split multi-values (e.g. ',', ';', '/')\n",
    "        top_n (int): Number of most frequent items to encode\n",
    "        prefix (str): Prefix to use for new columns (optional)\n",
    "        drop_original (bool): Whether to drop the original column after encoding\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with binary features added\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect all individual items from the column\n",
    "    all_items = []\n",
    "    df[column].dropna().apply(\n",
    "        lambda x: all_items.extend([\n",
    "            unidecode(item.strip().lower())\n",
    "            for item in str(x).split(separator) if item.strip()\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Count frequency and keep top-N most common items\n",
    "    item_counts = Counter(all_items)\n",
    "    top_items = [item for item, _ in item_counts.most_common(top_n)]\n",
    "\n",
    "    # Create a binary column for each top item\n",
    "    for item in top_items:\n",
    "        clean_item = re.sub(r'\\W+', '_', item)  # Sanitize column name\n",
    "        new_col_name = f\"{prefix or column}_{clean_item}\"\n",
    "        df[new_col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notnull(x) and item in [\n",
    "                unidecode(i.strip().lower())\n",
    "                for i in str(x).split(separator)\n",
    "            ] else 0\n",
    "        )\n",
    "\n",
    "    # Optionally drop the original column\n",
    "    if drop_original:\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "28a6b132d851ed13",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:06.692742Z",
     "start_time": "2025-06-09T05:02:06.627026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Processing multi-value text features ---\")\n",
    "\n",
    "# Process 'interests'\n",
    "if 'interests' in user_features_df.columns:\n",
    "    user_features_df['interests'] = user_features_df['interests'].fillna('')\n",
    "    # Lấy top items cho interests\n",
    "    all_interests_items = []\n",
    "    user_features_df['interests'].dropna().apply(\n",
    "        lambda x: all_interests_items.extend([\n",
    "            unidecode(item.strip().lower())\n",
    "            for item in str(x).split('-') if item.strip() # Dùng separator='-'\n",
    "        ])\n",
    "    )\n",
    "    interest_counts = Counter(all_interests_items)\n",
    "    top_n_interests_actual = 30 # Giống top_n đã dùng\n",
    "    top_interests_items_list = [item for item, _ in interest_counts.most_common(top_n_interests_actual)]\n",
    "    joblib.dump(top_interests_items_list, \"../models/top_interests_items.joblib\")\n",
    "    print(f\"Saved top_interests_items.joblib (Top {len(top_interests_items_list)} items)\")\n",
    "\n",
    "    user_features_df = create_multivalue_binary_features(\n",
    "        user_features_df, column='interests', separator='-',\n",
    "        top_n=top_n_interests_actual, prefix='interest'\n",
    "    )\n",
    "    print(\"Binary features created for 'interests'.\")\n",
    "\n",
    "# Process 'languages'\n",
    "if 'languages' in user_features_df.columns:\n",
    "    user_features_df['languages'] = user_features_df['languages'].fillna('')\n",
    "    # Lấy top items cho languages\n",
    "    all_languages_items = []\n",
    "    user_features_df['languages'].dropna().apply(\n",
    "        lambda x: all_languages_items.extend([\n",
    "            unidecode(item.strip().lower())\n",
    "            for item in str(x).split('-') if item.strip()\n",
    "        ])\n",
    "    )\n",
    "    language_counts = Counter(all_languages_items)\n",
    "    top_n_languages_actual = 15 # Giống top_n đã dùng\n",
    "    top_languages_items_list = [item for item, _ in language_counts.most_common(top_n_languages_actual)]\n",
    "    joblib.dump(top_languages_items_list, \"../models/top_languages_items.joblib\")\n",
    "    print(f\"Saved top_languages_items.joblib (Top {len(top_languages_items_list)} items)\")\n",
    "\n",
    "    user_features_df = create_multivalue_binary_features(\n",
    "        user_features_df, column='languages', separator='-',\n",
    "        top_n=top_n_languages_actual, prefix='lang'\n",
    "    )\n",
    "    print(\"Binary features created for 'languages'.\")\n",
    "\n",
    "# Process 'pets'\n",
    "if 'pets' in user_features_df.columns:\n",
    "    user_features_df['pets'] = user_features_df['pets'].fillna('')\n",
    "    # Lấy top items cho pets\n",
    "    all_pets_items = []\n",
    "    user_features_df['pets'].dropna().apply(\n",
    "        lambda x: all_pets_items.extend([\n",
    "            unidecode(item.strip().lower())\n",
    "            for item in str(x).split('-') if item.strip()\n",
    "        ])\n",
    "    )\n",
    "    pet_counts = Counter(all_pets_items)\n",
    "    top_n_pets_actual = 10 # Giống top_n đã dùng\n",
    "    top_pets_items_list = [item for item, _ in pet_counts.most_common(top_n_pets_actual)]\n",
    "    joblib.dump(top_pets_items_list, \"../models/top_pets_items.joblib\")\n",
    "    print(f\"Saved top_pets_items.joblib (Top {len(top_pets_items_list)} items)\")\n",
    "\n",
    "    user_features_df = create_multivalue_binary_features(\n",
    "        user_features_df, column='pets', separator='-',\n",
    "        top_n=top_n_pets_actual, prefix='pet'\n",
    "    )\n",
    "    print(\"Binary features created for 'pets'.\")"
   ],
   "id": "f9ae7f9dd9160832",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing multi-value text features ---\n",
      "Saved top_interests_items.joblib (Top 10 items)\n",
      "Binary features created for 'interests'.\n",
      "Saved top_languages_items.joblib (Top 15 items)\n",
      "Binary features created for 'languages'.\n",
      "Saved top_pets_items.joblib (Top 9 items)\n",
      "Binary features created for 'pets'.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:08.881826Z",
     "start_time": "2025-06-09T05:02:06.712293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process 'bio' text using TF-IDF vectorization (limited to top 100 features including unigrams and bigrams)\n",
    "if 'bio' in user_features_df.columns:\n",
    "    print(\"\\n--- Generating TF-IDF features from 'bio' ---\")\n",
    "\n",
    "    # Preprocess the text (e.g. lowercasing, removing punctuation, lemmatization)\n",
    "    user_features_df['processed_bio'] = user_features_df['bio'].apply(preprocess_text)\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer with a limit of 100 features and n-grams (1,2)\n",
    "    tfidf_vectorizer_bio = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "\n",
    "    # Vectorize the processed 'bio' text\n",
    "    bio_tfidf_matrix = tfidf_vectorizer_bio.fit_transform(user_features_df['processed_bio'])\n",
    "\n",
    "    # Convert the sparse matrix to a DataFrame with meaningful column names\n",
    "    bio_tfidf_df = pd.DataFrame(\n",
    "        bio_tfidf_matrix.toarray(),\n",
    "        columns=[f\"bio_tfidf_{i}\" for i in range(bio_tfidf_matrix.shape[1])]\n",
    "    )\n",
    "\n",
    "    # Combine the TF-IDF features with the user feature DataFrame\n",
    "    user_features_df = pd.concat([user_features_df.reset_index(drop=True), bio_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # --- Lưu TfidfVectorizer ---\n",
    "    joblib.dump(tfidf_vectorizer_bio, \"../models/tfidf_vectorizer_bio.joblib\")\n",
    "    print(\"Saved tfidf_vectorizer_bio.joblib\")\n",
    "\n",
    "    print(\"TF-IDF features extracted from 'bio' and added to the dataset.\")"
   ],
   "id": "a07b4f407bf8a4e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating TF-IDF features from 'bio' ---\n",
      "Saved tfidf_vectorizer_bio.joblib\n",
      "TF-IDF features extracted from 'bio' and added to the dataset.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:08.921037Z",
     "start_time": "2025-06-09T05:02:08.913805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2.1.4. Process Geographic Features ---\n",
    "print(\"\\n--- Processing Geographic Features ---\")\n",
    "\n",
    "# Handle 'location_preference' feature\n",
    "# Value -1 usually means 'no restriction' (e.g. willing to match anywhere)\n",
    "if 'location_preference' in user_features_df.columns:\n",
    "    # Create a binary flag for 'everywhere' preference\n",
    "    user_features_df['loc_pref_is_everywhere'] = (user_features_df['location_preference'] == -1).astype(int)\n",
    "\n",
    "    # Replace -1 with 0 or another logical value for distance (e.g., max distance)\n",
    "    user_features_df['location_preference_km'] = user_features_df['location_preference'].apply(\n",
    "        lambda x: 0 if x == -1 else x\n",
    "    )\n",
    "\n",
    "    # Normalize location preference distance\n",
    "    scaler_loc_pref = MinMaxScaler()\n",
    "    user_features_df['location_preference_km_scaled'] = scaler_loc_pref.fit_transform(\n",
    "        user_features_df[['location_preference_km']]\n",
    "    )\n",
    "    joblib.dump(scaler_loc_pref, \"../models/location_preference_scaler.joblib\")\n",
    "    print(\"Finished processing 'location_preference'.\")"
   ],
   "id": "2dcc8b2c04d80f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Geographic Features ---\n",
      "Finished processing 'location_preference'.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:08.948642Z",
     "start_time": "2025-06-09T05:02:08.942152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale latitude and longitude features if they exist\n",
    "# Usually these raw lat/lon values are less meaningful alone,\n",
    "# but scaling them can be useful if used directly as user features\n",
    "if 'latitude' in user_features_df.columns and 'longitude' in user_features_df.columns:\n",
    "    scaler_lat = MinMaxScaler()\n",
    "    user_features_df['latitude_scaled'] = scaler_lat.fit_transform(user_features_df[['latitude']])\n",
    "    joblib.dump(scaler_lat, \"../models/latitude_scaler.joblib\")\n",
    "\n",
    "    scaler_lon = MinMaxScaler()\n",
    "    user_features_df['longitude_scaled'] = scaler_lon.fit_transform(user_features_df[['longitude']])\n",
    "    joblib.dump(scaler_lon, \"../models/longitude_scaler.joblib\")\n",
    "\n",
    "    print(\"Scaled 'latitude' and 'longitude'.\")"
   ],
   "id": "816aa66f6dce7031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled 'latitude' and 'longitude'.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:09.126994Z",
     "start_time": "2025-06-09T05:02:08.981708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select final user feature columns by dropping raw or unnecessary columns\n",
    "# These columns are either original raw data, personal info, or text fields\n",
    "cols_to_drop_from_user_features = [\n",
    "    'username', 'password', 'email', 'phone_number', 'first_name', 'last_name',\n",
    "    'date_of_birth', 'height', 'age', 'bio', 'processed_bio',\n",
    "    'interests', 'languages', 'pets', 'country', 'state', 'city',  # not used directly\n",
    "    'latitude', 'longitude', 'location_preference', 'location_preference_km'\n",
    "]\n",
    "\n",
    "# If 'job' and 'education_level' columns exist in original profiles_df,\n",
    "# drop them here because they've been encoded as top-N features\n",
    "if 'job' in profiles_df.columns:\n",
    "    cols_to_drop_from_user_features.append('job')\n",
    "if 'education_level' in profiles_df.columns:\n",
    "    cols_to_drop_from_user_features.append('education_level')\n",
    "\n",
    "# Drop specified columns, ignoring errors if some columns don't exist\n",
    "user_features_final_df = user_features_df.drop(columns=cols_to_drop_from_user_features, errors='ignore')\n",
    "\n",
    "# Set 'id' column as index for easy lookup and merging later\n",
    "user_features_final_df.set_index('id', inplace=True)\n",
    "\n",
    "# --- Lưu danh sách cột User Features Final ---\n",
    "user_features_final_columns_list = user_features_final_df.columns.tolist()\n",
    "joblib.dump(user_features_final_columns_list, \"../models/user_features_final_columns.joblib\")\n",
    "print(\"Saved user_features_final_columns.joblib\")\n",
    "\n",
    "print(\"\\n--- Final User Features Columns ---\")\n",
    "print(user_features_final_df.info())\n",
    "print(user_features_final_df.head())\n",
    "\n",
    "# Save the engineered user features for later use in pairwise feature creation\n",
    "user_features_final_df.to_csv(\"../data/user_features_engineered.csv\")\n",
    "print(\"Saved user_features_engineered.csv\")"
   ],
   "id": "39a2583c5ca59997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved user_features_final_columns.joblib\n",
      "\n",
      "--- Final User Features Columns ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2001 entries, 1 to 2001\n",
      "Columns: 187 entries, interested_in_new_language to longitude_scaled\n",
      "dtypes: float64(129), int64(58)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "    interested_in_new_language  dropped_out_school  age_scaled  height_scaled  \\\n",
      "id                                                                              \n",
      "1                            1                   0    0.173077       0.392857   \n",
      "2                            1                   0    0.153846       0.464286   \n",
      "3                            1                   0    0.057692       0.500000   \n",
      "4                            0                   0    0.192308       0.464286   \n",
      "5                            0                   1    0.096154       0.464286   \n",
      "\n",
      "    sex_female  sex_male  sex_non-binary  sex_prefer not to say  \\\n",
      "id                                                                \n",
      "1          1.0       0.0             0.0                    0.0   \n",
      "2          0.0       1.0             0.0                    0.0   \n",
      "3          1.0       0.0             0.0                    0.0   \n",
      "4          0.0       1.0             0.0                    0.0   \n",
      "5          1.0       0.0             0.0                    0.0   \n",
      "\n",
      "    orientation_bisexual  orientation_homosexual  ...  bio_tfidf_94  \\\n",
      "id                                                ...                 \n",
      "1                    0.0                     1.0  ...       0.33191   \n",
      "2                    0.0                     1.0  ...       0.00000   \n",
      "3                    1.0                     0.0  ...       0.00000   \n",
      "4                    1.0                     0.0  ...       0.00000   \n",
      "5                    1.0                     0.0  ...       0.00000   \n",
      "\n",
      "    bio_tfidf_95  bio_tfidf_96  bio_tfidf_97  bio_tfidf_98  bio_tfidf_99  \\\n",
      "id                                                                         \n",
      "1       0.122901           0.0      0.000000      0.000000           0.0   \n",
      "2       0.000000           0.0      0.000000      0.000000           0.0   \n",
      "3       0.151367           0.0      0.353485      0.000000           0.0   \n",
      "4       0.000000           0.0      0.000000      0.646626           0.0   \n",
      "5       0.000000           0.0      0.000000      0.000000           0.0   \n",
      "\n",
      "    loc_pref_is_everywhere  location_preference_km_scaled  latitude_scaled  \\\n",
      "id                                                                           \n",
      "1                        0                           0.60         0.259678   \n",
      "2                        0                           0.05         0.211399   \n",
      "3                        0                           0.70         0.878959   \n",
      "4                        0                           0.30         0.472307   \n",
      "5                        1                           0.00         0.489057   \n",
      "\n",
      "    longitude_scaled  \n",
      "id                    \n",
      "1           0.737810  \n",
      "2           0.833485  \n",
      "3           0.752219  \n",
      "4           0.677029  \n",
      "5           0.543499  \n",
      "\n",
      "[5 rows x 187 columns]\n",
      "Saved user_features_engineered.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:09.163495Z",
     "start_time": "2025-06-09T05:02:09.159114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the Haversine distance (in kilometers) between two geographic points\n",
    "    specified by their latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    - lat1, lon1: latitude and longitude of point 1\n",
    "    - lat2, lon2: latitude and longitude of point 2\n",
    "\n",
    "    Returns:\n",
    "    - Distance in kilometers as float\n",
    "    - np.nan if any coordinate is missing\n",
    "    \"\"\"\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return np.nan  # Return NaN if any coordinate is missing\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "\n",
    "def orientation_compatibility(sex1, orientation1, sex2, orientation2):\n",
    "    \"\"\"\n",
    "    Determine if two users' sexual orientations are compatible.\n",
    "\n",
    "    The logic is:\n",
    "    - User1 is interested in User2 if User2's sex matches User1's orientation preference.\n",
    "    - User2 is interested in User1 if User1's sex matches User2's orientation preference.\n",
    "    - If both users are interested in each other, they are considered compatible.\n",
    "    - Special handling if any value is 'prefer not to say'.\n",
    "\n",
    "    Parameters:\n",
    "    - sex1, sex2: string values of users' sex (e.g., 'male', 'female', 'non-binary', etc.)\n",
    "    - orientation1, orientation2: string values of users' sexual orientation\n",
    "      (e.g., 'straight', 'homosexual', 'bisexual', 'prefer not to say')\n",
    "\n",
    "    Returns:\n",
    "    - True if compatible, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def is_interested(sex_a, orientation_a, sex_b):\n",
    "        \"\"\"Helper to check if user A is interested in user B based on sex and orientation.\"\"\"\n",
    "        if orientation_a == 'straight':\n",
    "            # Straight males interested in females and vice versa\n",
    "            if (sex_a == 'male' and sex_b == 'female') or (sex_a == 'female' and sex_b == 'male'):\n",
    "                return True\n",
    "            # Non-binary straight users interested in non-non-binary partners\n",
    "            if sex_a == 'non-binary' and sex_b != 'non-binary':\n",
    "                return True\n",
    "            return False\n",
    "        elif orientation_a == 'homosexual':\n",
    "            # Interested if same sex\n",
    "            return sex_a == sex_b\n",
    "        elif orientation_a == 'bisexual':\n",
    "            # Interested in all sexes\n",
    "            return True\n",
    "        # Could add more orientations here\n",
    "        return False\n",
    "\n",
    "    # Check if either user prefers not to disclose, treat bisexual and prefer not to say as compatible\n",
    "    if ('prefer not to say' in {sex1, sex2, orientation1, orientation2}):\n",
    "        if (orientation1 in ['bisexual', 'prefer not to say'] and\n",
    "            orientation2 in ['bisexual', 'prefer not to say']):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Check mutual interest\n",
    "    user1_likes_user2 = is_interested(sex1, orientation1, sex2)\n",
    "    user2_likes_user1 = is_interested(sex2, orientation2, sex1)\n",
    "\n",
    "    return user1_likes_user2 and user2_likes_user1\n",
    "\n",
    "\n",
    "def jaccard_similarity(list1_str, list2_str, separator='-'):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between two multi-valued string features.\n",
    "\n",
    "    Each string contains multiple values separated by a separator (e.g., '-').\n",
    "    This function converts them into sets of normalized tokens and computes similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - list1_str, list2_str: strings of multi-valued features, e.g., 'hiking-swimming-movies'\n",
    "    - separator: character separating the values in the string\n",
    "\n",
    "    Returns:\n",
    "    - Jaccard similarity as a float between 0 and 1\n",
    "    \"\"\"\n",
    "    if pd.isna(list1_str) or pd.isna(list2_str):\n",
    "        return 0.0\n",
    "\n",
    "    set1 = set(item.strip().lower() for item in str(list1_str).split(separator) if item.strip())\n",
    "    set2 = set(item.strip().lower() for item in str(list2_str).split(separator) if item.strip())\n",
    "\n",
    "    if not set1 and not set2:\n",
    "        # If both sets empty, similarity could be defined as 1 or 0\n",
    "        return 0.0\n",
    "\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "\n",
    "    return intersection_size / union_size if union_size != 0 else 0.0"
   ],
   "id": "d2a8352699130dd",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:26.323761Z",
     "start_time": "2025-06-09T05:02:09.194396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- 2.2 Creating Pairwise Features ---\")\n",
    "\n",
    "pairwise_features_list = []\n",
    "\n",
    "# Convert profiles_df into a dictionary for fast access by user id\n",
    "profiles_info_dict = profiles_df.set_index('id').to_dict('index')\n",
    "\n",
    "# Ensure user_features_final_df is indexed by 'id' for quick lookup\n",
    "# If not, uncomment the next line:\n",
    "# user_features_final_df = user_features_final_df.set_index('id')\n",
    "\n",
    "print(f\"Generating pairwise features for {len(training_data_df)} pairs...\")\n",
    "\n",
    "for index, row in training_data_df.iterrows():\n",
    "    user1_id = int(row['user1'])\n",
    "    user2_id = int(row['user2'])\n",
    "    target = int(row['target'])\n",
    "\n",
    "    user1_profile = profiles_info_dict.get(user1_id)\n",
    "    user2_profile = profiles_info_dict.get(user2_id)\n",
    "\n",
    "    if user1_profile is None or user2_profile is None:\n",
    "        print(f\"Warning: Missing profile for user {user1_id} or {user2_id}. Skipping this pair.\")\n",
    "        continue\n",
    "\n",
    "    pair_features = {\n",
    "        'user1': user1_id,\n",
    "        'user2': user2_id,\n",
    "        'target': target\n",
    "    }\n",
    "\n",
    "    # 1. Basic differences or similarities: absolute difference in age, height\n",
    "    pair_features['age_diff'] = abs(user1_profile.get('age', 0) - user2_profile.get('age', 0))\n",
    "    pair_features['height_diff'] = abs(user1_profile.get('height', 0) - user2_profile.get('height', 0))\n",
    "\n",
    "    # 2. Geographical distance (in kilometers) between users\n",
    "    pair_features['geo_distance_km'] = haversine_distance(\n",
    "        user1_profile.get('latitude'), user1_profile.get('longitude'),\n",
    "        user2_profile.get('latitude'), user2_profile.get('longitude')\n",
    "    )\n",
    "    # Handle missing geo distances by assigning a large default value (e.g., 10,000 km)\n",
    "    if pd.isna(pair_features['geo_distance_km']):\n",
    "        pair_features['geo_distance_km'] = 10000.0\n",
    "\n",
    "    # 3. Location preference compatibility:\n",
    "    # Assume location_preference stores max distance user prefers (-1 means no preference)\n",
    "    user1_loc_pref = user1_profile.get('location_preference', -1)\n",
    "    user2_loc_pref = user2_profile.get('location_preference', -1)\n",
    "    dist = pair_features['geo_distance_km']\n",
    "\n",
    "    pair_features['user1_within_user2_loc_pref'] = int(user2_loc_pref == -1 or (dist is not None and dist <= user2_loc_pref))\n",
    "    pair_features['user2_within_user1_loc_pref'] = int(user1_loc_pref == -1 or (dist is not None and dist <= user1_loc_pref))\n",
    "\n",
    "    # 4. Sexual orientation compatibility (both directions)\n",
    "    pair_features['orientation_compatible_user1_to_user2'] = orientation_compatibility(\n",
    "        user1_profile.get('sex'), user1_profile.get('orientation'),\n",
    "        user2_profile.get('sex'), user2_profile.get('orientation')\n",
    "    )\n",
    "    pair_features['orientation_compatible_user2_to_user1'] = orientation_compatibility(\n",
    "        user2_profile.get('sex'), user2_profile.get('orientation'),\n",
    "        user1_profile.get('sex'), user1_profile.get('orientation')\n",
    "    )\n",
    "    # Final compatibility flag: True if either direction is compatible\n",
    "    pair_features['orientation_compatible_final'] = max(\n",
    "        pair_features['orientation_compatible_user1_to_user2'],\n",
    "        pair_features['orientation_compatible_user2_to_user1']\n",
    "    )\n",
    "\n",
    "    # 5. Similar habits: drinking and smoking match (binary)\n",
    "    pair_features['drink_match'] = int(user1_profile.get('drink') == user2_profile.get('drink'))\n",
    "    pair_features['smoke_match'] = int(user1_profile.get('smoke') == user2_profile.get('smoke'))\n",
    "\n",
    "    # 6. Education level match (simple binary: 1 if equal, else 0)\n",
    "    pair_features['education_match'] = int(user1_profile.get('education_level') == user2_profile.get('education_level'))\n",
    "\n",
    "    # 7. Jaccard similarity for interests\n",
    "    pair_features['interests_jaccard'] = jaccard_similarity(\n",
    "        user1_profile.get('interests'), user2_profile.get('interests'), separator='-'\n",
    "    )\n",
    "\n",
    "    # 8. Jaccard similarity for languages and shared interest in learning new languages\n",
    "    pair_features['languages_jaccard'] = jaccard_similarity(\n",
    "        user1_profile.get('languages'), user2_profile.get('languages'), separator='-'\n",
    "    )\n",
    "    pair_features['user1_wants_learn_lang'] = int(user1_profile.get('interested_in_new_language', 0))\n",
    "    pair_features['user2_wants_learn_lang'] = int(user2_profile.get('interested_in_new_language', 0))\n",
    "    pair_features['language_interest_match'] = int(pair_features['user1_wants_learn_lang'] == 1 and pair_features['user2_wants_learn_lang'] == 1)\n",
    "\n",
    "    # 9. Jaccard similarity for pets\n",
    "    pair_features['pets_jaccard'] = jaccard_similarity(\n",
    "        user1_profile.get('pets'), user2_profile.get('pets'), separator='-'\n",
    "    )\n",
    "\n",
    "    # 10. Similarity of user feature vectors (cosine similarity and mean absolute error)\n",
    "    try:\n",
    "        vec1 = user_features_final_df.loc[user1_id].values.reshape(1, -1)\n",
    "        vec2 = user_features_final_df.loc[user2_id].values.reshape(1, -1)\n",
    "\n",
    "        # Replace NaNs with zero for similarity computation\n",
    "        vec1 = np.nan_to_num(vec1, nan=0.0)\n",
    "        vec2 = np.nan_to_num(vec2, nan=0.0)\n",
    "\n",
    "        pair_features['user_features_cosine_sim'] = cosine_similarity(vec1, vec2)[0, 0]\n",
    "        pair_features['user_features_mae_diff'] = np.mean(np.abs(vec1 - vec2))\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Missing feature vector for user {user1_id} or {user2_id}. Using default similarity values.\")\n",
    "        pair_features['user_features_cosine_sim'] = 0.0\n",
    "        pair_features['user_features_mae_diff'] = 1.0  # Max diff as fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing feature vector similarity for pair ({user1_id}, {user2_id}): {e}\")\n",
    "        pair_features['user_features_cosine_sim'] = 0.0\n",
    "        pair_features['user_features_mae_diff'] = 1.0\n",
    "\n",
    "    pairwise_features_list.append(pair_features)\n",
    "\n",
    "    # Progress print every 5000 pairs\n",
    "    if (index + 1) % 5000 == 0:\n",
    "        print(f\"Processed {index + 1} / {len(training_data_df)} pairs...\")"
   ],
   "id": "48468f2c3642dac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.2 Creating Pairwise Features ---\n",
      "Generating pairwise features for 49758 pairs...\n",
      "Processed 5000 / 49758 pairs...\n",
      "Processed 10000 / 49758 pairs...\n",
      "Processed 15000 / 49758 pairs...\n",
      "Processed 20000 / 49758 pairs...\n",
      "Processed 25000 / 49758 pairs...\n",
      "Processed 30000 / 49758 pairs...\n",
      "Processed 35000 / 49758 pairs...\n",
      "Processed 40000 / 49758 pairs...\n",
      "Processed 45000 / 49758 pairs...\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:26.527350Z",
     "start_time": "2025-06-09T05:02:26.386663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the list of pairwise feature dictionaries to a DataFrame\n",
    "pairwise_features_df = pd.DataFrame(pairwise_features_list)\n",
    "\n",
    "pairwise_model_input_columns_list = [\n",
    "    col for col in pairwise_features_df.columns if col not in ['user1', 'user2', 'target']\n",
    "]\n",
    "joblib.dump(pairwise_model_input_columns_list, \"../models/pairwise_model_input_columns.joblib\")\n",
    "\n",
    "print(\"\\n--- First 5 rows of the pairwise features DataFrame ---\")\n",
    "print(pairwise_features_df.head())"
   ],
   "id": "ddff2eabdf1ca523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First 5 rows of the pairwise features DataFrame ---\n",
      "   user1  user2  target  age_diff  height_diff  geo_distance_km  \\\n",
      "0    259    961       0        18          6.0        22.675100   \n",
      "1    260    939       1        12          5.0        18.469857   \n",
      "2    927     62       0        11          6.0        19.247655   \n",
      "3    415   1353       0         4          1.0        15.782432   \n",
      "4    293   1667       1         7          7.0        14.220504   \n",
      "\n",
      "   user1_within_user2_loc_pref  user2_within_user1_loc_pref  \\\n",
      "0                            1                            1   \n",
      "1                            1                            1   \n",
      "2                            1                            1   \n",
      "3                            1                            1   \n",
      "4                            1                            1   \n",
      "\n",
      "   orientation_compatible_user1_to_user2  \\\n",
      "0                                   True   \n",
      "1                                   True   \n",
      "2                                   True   \n",
      "3                                   True   \n",
      "4                                   True   \n",
      "\n",
      "   orientation_compatible_user2_to_user1  ...  smoke_match  education_match  \\\n",
      "0                                   True  ...            0                0   \n",
      "1                                   True  ...            0                1   \n",
      "2                                   True  ...            1                1   \n",
      "3                                   True  ...            1                0   \n",
      "4                                   True  ...            0                0   \n",
      "\n",
      "   interests_jaccard  languages_jaccard  user1_wants_learn_lang  \\\n",
      "0               0.25                0.2                       0   \n",
      "1               0.00                1.0                       0   \n",
      "2               0.00                0.5                       1   \n",
      "3               0.50                1.0                       1   \n",
      "4               0.00                0.5                       0   \n",
      "\n",
      "   user2_wants_learn_lang  language_interest_match  pets_jaccard  \\\n",
      "0                       1                        0           0.0   \n",
      "1                       0                        0           0.0   \n",
      "2                       0                        0           0.0   \n",
      "3                       1                        1           0.0   \n",
      "4                       1                        0           0.0   \n",
      "\n",
      "   user_features_cosine_sim  user_features_mae_diff  \n",
      "0                  0.359217                0.131877  \n",
      "1                  0.352299                0.091437  \n",
      "2                  0.439483                0.128145  \n",
      "3                  0.500751                0.096154  \n",
      "4                  0.281672                0.135141  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:26.581224Z",
     "start_time": "2025-06-09T05:02:26.562621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize pairwise features (excluding 'user1', 'user2', and 'target' columns)\n",
    "\n",
    "# Identify columns to scale: all except 'user1', 'user2', 'target'\n",
    "cols_to_scale_pairwise = [col for col in pairwise_features_df.columns if col not in ['user1', 'user2', 'target']]\n",
    "\n",
    "# From these columns, select only the numerical ones for scaling\n",
    "numerical_cols_to_scale_pairwise = pairwise_features_df[cols_to_scale_pairwise].select_dtypes(include=np.number).columns\n",
    "\n",
    "if not numerical_cols_to_scale_pairwise.empty:\n",
    "    # Initialize a scaler (MinMaxScaler is used here; alternatively, StandardScaler can be used)\n",
    "    scaler_pairwise = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler on the numerical columns and transform the data accordingly\n",
    "    pairwise_features_df[numerical_cols_to_scale_pairwise] = scaler_pairwise.fit_transform(pairwise_features_df[numerical_cols_to_scale_pairwise])\n",
    "    joblib.dump(scaler_pairwise, \"../models/pairwise_features_scaler.joblib\")\n",
    "\n",
    "    print(f\"Normalized numerical columns in pairwise_features_df: {list(numerical_cols_to_scale_pairwise)}\")\n",
    "else:\n",
    "    print(\"No numerical columns to normalize in pairwise_features_df (excluding 'user1', 'user2', and 'target').\")"
   ],
   "id": "205c897a9a321157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized numerical columns in pairwise_features_df: ['age_diff', 'height_diff', 'geo_distance_km', 'user1_within_user2_loc_pref', 'user2_within_user1_loc_pref', 'drink_match', 'smoke_match', 'education_match', 'interests_jaccard', 'languages_jaccard', 'user1_wants_learn_lang', 'user2_wants_learn_lang', 'language_interest_match', 'pets_jaccard', 'user_features_cosine_sim', 'user_features_mae_diff']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:02:27.168456Z",
     "start_time": "2025-06-09T05:02:26.621228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Information of pairwise_features_df after handling NaNs and normalization ---\")\n",
    "# Display summary info of the DataFrame, including number of non-null entries and data types\n",
    "pairwise_features_df.info()\n",
    "\n",
    "# Display first few rows of the DataFrame for a quick look at the data\n",
    "print(pairwise_features_df.head())\n",
    "\n",
    "# Save the processed pairwise features DataFrame to a CSV file\n",
    "pairwise_features_df.to_csv(\"../data/pairwise_features_engineered.csv\", index=False)\n",
    "print(\"Saved pairwise_features_engineered.csv\")\n",
    "\n",
    "print(\"\\n--- Phase 2 Completed ---\")\n",
    "print(\"Next step: Phase 3 - Model Building and Training\")\n",
    "print(\"The pairwise_features_df dataset is now ready for model training.\")\n"
   ],
   "id": "6a210815b1e21846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Information of pairwise_features_df after handling NaNs and normalization ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49758 entries, 0 to 49757\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   user1                                  49758 non-null  int64  \n",
      " 1   user2                                  49758 non-null  int64  \n",
      " 2   target                                 49758 non-null  int64  \n",
      " 3   age_diff                               49758 non-null  float64\n",
      " 4   height_diff                            49758 non-null  float64\n",
      " 5   geo_distance_km                        49758 non-null  float64\n",
      " 6   user1_within_user2_loc_pref            49758 non-null  float64\n",
      " 7   user2_within_user1_loc_pref            49758 non-null  float64\n",
      " 8   orientation_compatible_user1_to_user2  49758 non-null  bool   \n",
      " 9   orientation_compatible_user2_to_user1  49758 non-null  bool   \n",
      " 10  orientation_compatible_final           49758 non-null  bool   \n",
      " 11  drink_match                            49758 non-null  float64\n",
      " 12  smoke_match                            49758 non-null  float64\n",
      " 13  education_match                        49758 non-null  float64\n",
      " 14  interests_jaccard                      49758 non-null  float64\n",
      " 15  languages_jaccard                      49758 non-null  float64\n",
      " 16  user1_wants_learn_lang                 49758 non-null  float64\n",
      " 17  user2_wants_learn_lang                 49758 non-null  float64\n",
      " 18  language_interest_match                49758 non-null  float64\n",
      " 19  pets_jaccard                           49758 non-null  float64\n",
      " 20  user_features_cosine_sim               49758 non-null  float64\n",
      " 21  user_features_mae_diff                 49758 non-null  float64\n",
      "dtypes: bool(3), float64(16), int64(3)\n",
      "memory usage: 7.4 MB\n",
      "   user1  user2  target  age_diff  height_diff  geo_distance_km  \\\n",
      "0    259    961       0  0.352941     0.250000         0.265121   \n",
      "1    260    939       1  0.235294     0.208333         0.215792   \n",
      "2    927     62       0  0.215686     0.250000         0.224916   \n",
      "3    415   1353       0  0.078431     0.041667         0.184267   \n",
      "4    293   1667       1  0.137255     0.291667         0.165945   \n",
      "\n",
      "   user1_within_user2_loc_pref  user2_within_user1_loc_pref  \\\n",
      "0                          1.0                          1.0   \n",
      "1                          1.0                          1.0   \n",
      "2                          1.0                          1.0   \n",
      "3                          1.0                          1.0   \n",
      "4                          1.0                          1.0   \n",
      "\n",
      "   orientation_compatible_user1_to_user2  \\\n",
      "0                                   True   \n",
      "1                                   True   \n",
      "2                                   True   \n",
      "3                                   True   \n",
      "4                                   True   \n",
      "\n",
      "   orientation_compatible_user2_to_user1  ...  smoke_match  education_match  \\\n",
      "0                                   True  ...          0.0              0.0   \n",
      "1                                   True  ...          0.0              1.0   \n",
      "2                                   True  ...          1.0              1.0   \n",
      "3                                   True  ...          1.0              0.0   \n",
      "4                                   True  ...          0.0              0.0   \n",
      "\n",
      "   interests_jaccard  languages_jaccard  user1_wants_learn_lang  \\\n",
      "0               0.25           0.066667                     0.0   \n",
      "1               0.00           1.000000                     0.0   \n",
      "2               0.00           0.416667                     1.0   \n",
      "3               0.50           1.000000                     1.0   \n",
      "4               0.00           0.416667                     0.0   \n",
      "\n",
      "   user2_wants_learn_lang  language_interest_match  pets_jaccard  \\\n",
      "0                     1.0                      0.0           0.0   \n",
      "1                     0.0                      0.0           0.0   \n",
      "2                     0.0                      0.0           0.0   \n",
      "3                     1.0                      1.0           0.0   \n",
      "4                     1.0                      0.0           0.0   \n",
      "\n",
      "   user_features_cosine_sim  user_features_mae_diff  \n",
      "0                  0.378231                0.548072  \n",
      "1                  0.368004                0.279688  \n",
      "2                  0.496894                0.523303  \n",
      "3                  0.587471                0.310992  \n",
      "4                  0.263591                0.569730  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Saved pairwise_features_engineered.csv\n",
      "\n",
      "--- Phase 2 Completed ---\n",
      "Next step: Phase 3 - Model Building and Training\n",
      "The pairwise_features_df dataset is now ready for model training.\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
