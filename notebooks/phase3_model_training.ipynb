{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.457547Z",
     "start_time": "2025-06-09T05:12:08.943021Z"
    }
   },
   "source": [
    "import pandas as pd  # Data manipulation and analysis library\n",
    "import numpy as np   # Numerical computing library\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting data and hyperparameter tuning\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
    "import lightgbm as lgb  # LightGBM gradient boosting framework\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,    # Accuracy metric\n",
    "    precision_score,   # Precision metric\n",
    "    recall_score,      # Recall metric\n",
    "    f1_score,          # F1-score metric\n",
    "    roc_auc_score,     # ROC AUC metric\n",
    "    classification_report  # Detailed classification report\n",
    ")\n",
    "\n",
    "import joblib  # For saving and loading machine learning models\n",
    "\n",
    "import json  # For working with JSON data format"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.573640Z",
     "start_time": "2025-06-09T05:12:09.486607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load data from Stage 2 (Assuming it has been run and saved) ---\n",
    "print(\"--- Stage 3: Model Building and Training ---\")\n",
    "print(\"\\n--- 3.0 Load processed pairwise features data from Stage 2 ---\")\n",
    "\n",
    "try:\n",
    "    # Attempt to load the saved pairwise features file from Stage 2\n",
    "    pairwise_features_df = pd.read_csv(\"../data/pairwise_features_engineered.csv\")\n",
    "    print(\"Successfully loaded pairwise features data!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File pairwise_features_engineered.csv not found.\")\n",
    "    print(\"Please ensure that Stage 2 has been run and the file has been saved, or the data exists in memory.\")"
   ],
   "id": "98043f5966a31e52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3: Model Building and Training ---\n",
      "\n",
      "--- 3.0 Load processed pairwise features data from Stage 2 ---\n",
      "Successfully loaded pairwise features data!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.711849Z",
     "start_time": "2025-06-09T05:12:09.698547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of samples in pairwise_features_df: {len(pairwise_features_df)}\")\n",
    "print(\"Information about pairwise_features_df:\")\n",
    "pairwise_features_df.info()"
   ],
   "id": "7ac2e23d723dac37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in pairwise_features_df: 49758\n",
      "Information about pairwise_features_df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49758 entries, 0 to 49757\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   user1                                  49758 non-null  int64  \n",
      " 1   user2                                  49758 non-null  int64  \n",
      " 2   target                                 49758 non-null  int64  \n",
      " 3   age_diff                               49758 non-null  float64\n",
      " 4   height_diff                            49758 non-null  float64\n",
      " 5   geo_distance_km                        49758 non-null  float64\n",
      " 6   user1_within_user2_loc_pref            49758 non-null  float64\n",
      " 7   user2_within_user1_loc_pref            49758 non-null  float64\n",
      " 8   orientation_compatible_user1_to_user2  49758 non-null  bool   \n",
      " 9   orientation_compatible_user2_to_user1  49758 non-null  bool   \n",
      " 10  orientation_compatible_final           49758 non-null  bool   \n",
      " 11  drink_match                            49758 non-null  float64\n",
      " 12  smoke_match                            49758 non-null  float64\n",
      " 13  education_match                        49758 non-null  float64\n",
      " 14  interests_jaccard                      49758 non-null  float64\n",
      " 15  languages_jaccard                      49758 non-null  float64\n",
      " 16  user1_wants_learn_lang                 49758 non-null  float64\n",
      " 17  user2_wants_learn_lang                 49758 non-null  float64\n",
      " 18  language_interest_match                49758 non-null  float64\n",
      " 19  pets_jaccard                           49758 non-null  float64\n",
      " 20  user_features_cosine_sim               49758 non-null  float64\n",
      " 21  user_features_mae_diff                 49758 non-null  float64\n",
      "dtypes: bool(3), float64(16), int64(3)\n",
      "memory usage: 7.4 MB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.760487Z",
     "start_time": "2025-06-09T05:12:09.755315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.1 Dataset Splitting ---\n",
    "print(\"\\n--- 3.1 Dataset Splitting ---\")\n",
    "\n",
    "# Remove non-feature columns (user1 and user2 can be kept for identification but excluded from features X)\n",
    "if 'user1' in pairwise_features_df.columns and 'user2' in pairwise_features_df.columns:\n",
    "    X = pairwise_features_df.drop(columns=['target', 'user1', 'user2'], errors='ignore')\n",
    "else:\n",
    "    X = pairwise_features_df.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "y = pairwise_features_df['target']"
   ],
   "id": "12587389062e2d0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3.1 Dataset Splitting ---\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.812022Z",
     "start_time": "2025-06-09T05:12:09.780421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the dataset into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split the training set into final training and validation sets\n",
    "# (e.g., 80% train_final, 20% validation from the original training set)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ],
   "id": "b80a2d4736f43c8a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.941717Z",
     "start_time": "2025-06-09T05:12:09.830231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Training set size (train_final): {X_train_final.shape}, {y_train_final.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Save test sets for later use\n",
    "X_test.to_csv(\"../data/X_test_data.csv\", index=False)\n",
    "y_test.to_csv(\"../data/y_test_data.csv\", index=False)"
   ],
   "id": "9a2ced6fd562821a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (train_final): (31844, 19), (31844,)\n",
      "Validation set size: (7962, 19), (7962,)\n",
      "Test set size: (9952, 19), (9952,)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:09.963919Z",
     "start_time": "2025-06-09T05:12:09.961153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2 Model Selection and Training ---\n",
    "print(\"\\n--- 3.2 Model Selection and Training ---\")\n",
    "\n",
    "models = {}\n",
    "model_predictions = {}\n",
    "model_probabilities = {}"
   ],
   "id": "13a3c49afa6ccda3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3.2 Model Selection and Training ---\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:10.114028Z",
     "start_time": "2025-06-09T05:12:10.001583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2.1 Logistic Regression ---\n",
    "print(\"\\n--- Training Logistic Regression (default parameters initially) ---\")\n",
    "log_reg_default = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "log_reg_default.fit(X_train_final, y_train_final)\n",
    "models['Logistic Regression_default'] = log_reg_default\n",
    "\n",
    "y_pred_log_reg_val_default = log_reg_default.predict(X_val)\n",
    "y_proba_log_reg_val_default = log_reg_default.predict_proba(X_val)[:, 1]\n",
    "\n",
    "model_predictions['Logistic Regression_default_val'] = y_pred_log_reg_val_default\n",
    "model_probabilities['Logistic Regression_default_val'] = y_proba_log_reg_val_default\n",
    "\n",
    "print(\"Logistic Regression (default) training completed.\")"
   ],
   "id": "a132bbff42ee93f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression (default parameters initially) ---\n",
      "Logistic Regression (default) training completed.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:11.017550Z",
     "start_time": "2025-06-09T05:12:10.139963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2.2 Random Forest Classifier ---\n",
    "print(\"\\n--- Training Random Forest Classifier (default parameters initially) ---\")\n",
    "rf_clf_default = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf_default.fit(X_train_final, y_train_final)\n",
    "models['Random Forest_default'] = rf_clf_default\n",
    "\n",
    "y_pred_rf_val_default = rf_clf_default.predict(X_val)\n",
    "y_proba_rf_val_default = rf_clf_default.predict_proba(X_val)[:, 1]\n",
    "\n",
    "model_predictions['Random Forest_default_val'] = y_pred_rf_val_default\n",
    "model_probabilities['Random Forest_default_val'] = y_proba_rf_val_default\n",
    "\n",
    "print(\"Random Forest (default) training completed.\")"
   ],
   "id": "7260645da90ae399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Random Forest Classifier (default parameters initially) ---\n",
      "Random Forest (default) training completed.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:11.201504Z",
     "start_time": "2025-06-09T05:12:11.058916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2.3 LightGBM Classifier ---\n",
    "print(\"\\n--- Training LightGBM Classifier (default parameters initially) ---\")\n",
    "lgbm_clf_default = lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm_clf_default.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(10, verbose=False)]\n",
    ")\n",
    "models['LightGBM_default'] = lgbm_clf_default\n",
    "\n",
    "y_pred_lgbm_val_default = lgbm_clf_default.predict(X_val)\n",
    "y_proba_lgbm_val_default = lgbm_clf_default.predict_proba(X_val)[:, 1]\n",
    "\n",
    "model_predictions['LightGBM_default_val'] = y_pred_lgbm_val_default\n",
    "model_probabilities['LightGBM_default_val'] = y_proba_lgbm_val_default\n",
    "\n",
    "print(\"LightGBM (default) training completed.\")"
   ],
   "id": "24b1704d2a6fb3fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM Classifier (default parameters initially) ---\n",
      "[LightGBM] [Info] Number of positive: 10615, number of negative: 21229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 877\n",
      "[LightGBM] [Info] Number of data points in the train set: 31844, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "LightGBM (default) training completed.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:12:32.737429Z",
     "start_time": "2025-06-09T05:12:11.310477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2.4 Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"\\n--- 3.2.4 Hyperparameter Tuning with GridSearchCV ---\")\n",
    "# Define a common scoring metric, e.g., roc_auc or f1. 'roc_auc' is good for imbalanced binary classification.\n",
    "# For precision/recall focus, 'f1' might be better.\n",
    "SCORING_METRIC = 'roc_auc' # Or 'f1', 'accuracy', etc.\n",
    "CV_FOLDS = 3 # Number of cross-validation folds. Increase for more robust tuning, but takes longer.\n",
    "\n",
    "# --- Hyperparameter Tuning for Logistic Regression ---\n",
    "print(f\"\\n--- Tuning Logistic Regression (scoring: {SCORING_METRIC}) ---\")\n",
    "param_grid_log_reg = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],       # Norm used in the penalization\n",
    "    # solver='liblinear' is good for l1/l2 and small datasets\n",
    "}\n",
    "grid_search_log_reg = GridSearchCV(\n",
    "    estimator=LogisticRegression(solver='liblinear', random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    param_grid=param_grid_log_reg,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=SCORING_METRIC,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_log_reg.fit(X_train_final, y_train_final)\n",
    "print(f\"Best parameters for Logistic Regression: {grid_search_log_reg.best_params_}\")\n",
    "print(f\"Best {SCORING_METRIC} score for Logistic Regression: {grid_search_log_reg.best_score_:.4f}\")\n",
    "models['Logistic Regression_tuned'] = grid_search_log_reg.best_estimator_\n",
    "\n",
    "# Predictions on validation set with tuned model\n",
    "y_pred_log_reg_tuned_val = models['Logistic Regression_tuned'].predict(X_val)\n",
    "y_proba_log_reg_tuned_val = models['Logistic Regression_tuned'].predict_proba(X_val)[:, 1]\n",
    "model_predictions['Logistic Regression_tuned_val'] = y_pred_log_reg_tuned_val\n",
    "model_probabilities['Logistic Regression_tuned_val'] = y_proba_log_reg_tuned_val"
   ],
   "id": "5bcc1416306a0ddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3.2.4 Hyperparameter Tuning with GridSearchCV ---\n",
      "\n",
      "--- Tuning Logistic Regression (scoring: roc_auc) ---\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Logistic Regression: {'C': 10, 'penalty': 'l2'}\n",
      "Best roc_auc score for Logistic Regression: 0.7431\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:14:07.873423Z",
     "start_time": "2025-06-09T05:12:32.876515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Hyperparameter Tuning for Random Forest ---\n",
    "print(f\"\\n--- Tuning Random Forest Classifier (scoring: {SCORING_METRIC}) ---\")\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],         # Number of trees\n",
    "    'max_depth': [None, 10, 20],          # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],    # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],      # Minimum number of samples required to be at a leaf node\n",
    "    # 'max_features': ['sqrt', 'log2'] # Number of features to consider when looking for the best split. 'auto' is sqrt\n",
    "}\n",
    "# Note: max_features='auto' is equivalent to 'sqrt' for RandomForestClassifier.\n",
    "# A smaller grid is used here for faster execution. Expand it for more thorough tuning.\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=SCORING_METRIC,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_rf.fit(X_train_final, y_train_final)\n",
    "print(f\"Best parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best {SCORING_METRIC} score for Random Forest: {grid_search_rf.best_score_:.4f}\")\n",
    "models['Random Forest_tuned'] = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predictions on validation set with tuned model\n",
    "y_pred_rf_tuned_val = models['Random Forest_tuned'].predict(X_val)\n",
    "y_proba_rf_tuned_val = models['Random Forest_tuned'].predict_proba(X_val)[:, 1]\n",
    "model_predictions['Random Forest_tuned_val'] = y_pred_rf_tuned_val\n",
    "model_probabilities['Random Forest_tuned_val'] = y_proba_rf_tuned_val"
   ],
   "id": "b977cbda58bf303f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Random Forest Classifier (scoring: roc_auc) ---\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best roc_auc score for Random Forest: 0.7394\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:18:35.519786Z",
     "start_time": "2025-06-09T05:14:08.147697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Hyperparameter Tuning for LightGBM ---\n",
    "print(f\"\\n--- Tuning LightGBM Classifier (scoring: {SCORING_METRIC}) ---\")\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 40],          # max number of leaves in one tree\n",
    "    'max_depth': [-1, 10, 20],            # max tree depth for base learners, -1 means no limit\n",
    "    # 'min_child_samples': [10, 20, 30], # min number of data in one leaf\n",
    "    # 'subsample': [0.8, 0.9, 1.0],       # subsample ratio of the training instance\n",
    "    # 'colsample_bytree': [0.8, 0.9, 1.0] # subsample ratio of columns when constructing each tree\n",
    "}\n",
    "# A smaller grid is used here for faster execution. Expand it for more thorough tuning.\n",
    "# Note: For LightGBM, if using class_weight='balanced', you might also want to set boosting_type='gbdt' explicitly.\n",
    "# Early stopping is not directly used by GridSearchCV during its internal CV,\n",
    "# but the best_estimator_ will be a model with the specified n_estimators.\n",
    "# You could re-train the best estimator with early stopping on the full X_train_final and X_val if desired.\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    param_grid=param_grid_lgbm,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=SCORING_METRIC,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_lgbm.fit(X_train_final, y_train_final) # No eval_set or callbacks here, GridSearchCV handles CV\n",
    "print(f\"Best parameters for LightGBM: {grid_search_lgbm.best_params_}\")\n",
    "print(f\"Best {SCORING_METRIC} score for LightGBM: {grid_search_lgbm.best_score_:.4f}\")\n",
    "models['LightGBM_tuned'] = grid_search_lgbm.best_estimator_\n",
    "\n",
    "# Predictions on validation set with tuned model\n",
    "y_pred_lgbm_tuned_val = models['LightGBM_tuned'].predict(X_val)\n",
    "y_proba_lgbm_tuned_val = models['LightGBM_tuned'].predict_proba(X_val)[:, 1]\n",
    "model_predictions['LightGBM_tuned_val'] = y_pred_lgbm_tuned_val\n",
    "model_probabilities['LightGBM_tuned_val'] = y_proba_lgbm_tuned_val\n",
    "\n",
    "print(\"Hyperparameter tuning completed for all models.\")"
   ],
   "id": "150fdba290576d60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning LightGBM Classifier (scoring: roc_auc) ---\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21230, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 7077, number of negative: 14152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 876\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 7076, number of negative: 14153\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 21229, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000[LightGBM] [Info] Number of positive: 10615, number of negative: 21229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 877\n",
      "[LightGBM] [Info] Number of data points in the train set: 31844, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 200, 'num_leaves': 20}\n",
      "Best roc_auc score for LightGBM: 0.7375\n",
      "Hyperparameter tuning completed for all models.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:18:35.873454Z",
     "start_time": "2025-06-09T05:18:35.762083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- 3.3 Preliminary Evaluation on Validation Set ---\")\n",
    "\n",
    "# We now have default and tuned versions for each model.\n",
    "# The loop will iterate through all of them.\n",
    "all_model_names = list(models.keys()) # Get all model names (default and tuned)\n",
    "\n",
    "for model_name in all_model_names:\n",
    "    # Check if predictions for validation set exist\n",
    "    val_pred_key = model_name + \"_val\" # e.g., \"Logistic Regression_default_val\" or \"Logistic Regression_tuned_val\"\n",
    "\n",
    "    if val_pred_key in model_predictions:\n",
    "        y_pred = model_predictions[val_pred_key]\n",
    "        y_proba = model_probabilities[val_pred_key]\n",
    "\n",
    "        print(f\"\\n--- Results for {model_name} on Validation Set ---\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_val, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_val, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"F1 Score: {f1_score(y_val, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_val, y_proba):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0))\n",
    "    else:\n",
    "        print(f\"Validation predictions for {model_name} not found (key: {val_pred_key}). Skipping evaluation.\")"
   ],
   "id": "22db676911791502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3.3 Preliminary Evaluation on Validation Set ---\n",
      "\n",
      "--- Results for Logistic Regression_default on Validation Set ---\n",
      "Accuracy: 0.6350\n",
      "Precision: 0.4773\n",
      "Recall: 0.9996\n",
      "F1 Score: 0.6461\n",
      "ROC AUC: 0.7402\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62      5308\n",
      "           1       0.48      1.00      0.65      2654\n",
      "\n",
      "    accuracy                           0.64      7962\n",
      "   macro avg       0.74      0.73      0.63      7962\n",
      "weighted avg       0.83      0.64      0.63      7962\n",
      "\n",
      "\n",
      "--- Results for Random Forest_default on Validation Set ---\n",
      "Accuracy: 0.6623\n",
      "Precision: 0.4908\n",
      "Recall: 0.3512\n",
      "F1 Score: 0.4094\n",
      "ROC AUC: 0.7370\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      5308\n",
      "           1       0.49      0.35      0.41      2654\n",
      "\n",
      "    accuracy                           0.66      7962\n",
      "   macro avg       0.60      0.58      0.59      7962\n",
      "weighted avg       0.64      0.66      0.65      7962\n",
      "\n",
      "\n",
      "--- Results for LightGBM_default on Validation Set ---\n",
      "Accuracy: 0.6351\n",
      "Precision: 0.4774\n",
      "Recall: 0.9966\n",
      "F1 Score: 0.6455\n",
      "ROC AUC: 0.7422\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62      5308\n",
      "           1       0.48      1.00      0.65      2654\n",
      "\n",
      "    accuracy                           0.64      7962\n",
      "   macro avg       0.74      0.73      0.63      7962\n",
      "weighted avg       0.82      0.64      0.63      7962\n",
      "\n",
      "\n",
      "--- Results for Logistic Regression_tuned on Validation Set ---\n",
      "Accuracy: 0.6350\n",
      "Precision: 0.4773\n",
      "Recall: 0.9996\n",
      "F1 Score: 0.6461\n",
      "ROC AUC: 0.7400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62      5308\n",
      "           1       0.48      1.00      0.65      2654\n",
      "\n",
      "    accuracy                           0.64      7962\n",
      "   macro avg       0.74      0.73      0.63      7962\n",
      "weighted avg       0.83      0.64      0.63      7962\n",
      "\n",
      "\n",
      "--- Results for Random Forest_tuned on Validation Set ---\n",
      "Accuracy: 0.6358\n",
      "Precision: 0.4778\n",
      "Recall: 0.9981\n",
      "F1 Score: 0.6463\n",
      "ROC AUC: 0.7437\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62      5308\n",
      "           1       0.48      1.00      0.65      2654\n",
      "\n",
      "    accuracy                           0.64      7962\n",
      "   macro avg       0.74      0.73      0.64      7962\n",
      "weighted avg       0.82      0.64      0.63      7962\n",
      "\n",
      "\n",
      "--- Results for LightGBM_tuned on Validation Set ---\n",
      "Accuracy: 0.6353\n",
      "Precision: 0.4775\n",
      "Recall: 0.9981\n",
      "F1 Score: 0.6459\n",
      "ROC AUC: 0.7474\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62      5308\n",
      "           1       0.48      1.00      0.65      2654\n",
      "\n",
      "    accuracy                           0.64      7962\n",
      "   macro avg       0.74      0.73      0.63      7962\n",
      "weighted avg       0.82      0.64      0.63      7962\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:18:36.132499Z",
     "start_time": "2025-06-09T05:18:36.119498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.4 Determine and Save Best Model ---\n",
    "print(\"\\n--- 3.4 Determine and Save Best Model ---\")\n",
    "\n",
    "# 1. Collect validation scores (ROC AUC in this case)\n",
    "validation_roc_auc_scores = {}\n",
    "for model_name_key, proba_val in model_probabilities.items(): # Keys like \"Logistic Regression_default_val\"\n",
    "    if \"_val\" in model_name_key:\n",
    "        # Extract the base model name (e.g., \"Logistic Regression_default\")\n",
    "        # This base name should match the keys in the `models` dictionary\n",
    "        model_base_name = model_name_key.replace(\"_val\", \"\")\n",
    "        if model_base_name in models: # Ensure the model exists\n",
    "            score = roc_auc_score(y_val, proba_val)\n",
    "            validation_roc_auc_scores[model_base_name] = score\n",
    "            print(f\"Validation ROC AUC for {model_base_name}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Model {model_base_name} not found in 'models' dictionary, but has validation probabilities.\")"
   ],
   "id": "b87430e36af45f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3.4 Determine and Save Best Model ---\n",
      "Validation ROC AUC for Logistic Regression_default: 0.7402\n",
      "Validation ROC AUC for Random Forest_default: 0.7370\n",
      "Validation ROC AUC for LightGBM_default: 0.7422\n",
      "Validation ROC AUC for Logistic Regression_tuned: 0.7400\n",
      "Validation ROC AUC for Random Forest_tuned: 0.7437\n",
      "Validation ROC AUC for LightGBM_tuned: 0.7474\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:18:36.210494Z",
     "start_time": "2025-06-09T05:18:36.189798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Determine the best performing model based on ROC AUC on validation set\n",
    "best_overall_model_name = None\n",
    "best_overall_roc_auc = -1.0  # Initialize with a value lower than any possible ROC AUC\n",
    "\n",
    "if validation_roc_auc_scores: # Check if dictionary is not empty\n",
    "    # Find the model name with the highest ROC AUC score\n",
    "    best_overall_model_name = max(validation_roc_auc_scores, key=validation_roc_auc_scores.get)\n",
    "    best_overall_roc_auc = validation_roc_auc_scores[best_overall_model_name]\n",
    "else:\n",
    "    print(\"No validation scores collected. Cannot determine the best model.\")\n",
    "\n",
    "\n",
    "if best_overall_model_name and best_overall_model_name in models:\n",
    "    print(f\"\\nOverall best model on validation set: {best_overall_model_name} with ROC AUC: {best_overall_roc_auc:.4f}\")\n",
    "\n",
    "    # 3. Save the best model object\n",
    "    best_model_instance = models[best_overall_model_name]\n",
    "    model_filename = f\"../models/best_overall_model.joblib\" # Simplified filename\n",
    "    joblib.dump(best_model_instance, model_filename)\n",
    "    print(f\"Saved best model ({best_overall_model_name}) to {model_filename}\")\n",
    "\n",
    "    # 4. Save information about the best model\n",
    "    # Retrieve all metrics for the best model from the evaluation step (or re-calculate if needed)\n",
    "    best_model_val_pred_key = best_overall_model_name + \"_val\"\n",
    "    y_pred_best_val = model_predictions[best_model_val_pred_key]\n",
    "    y_proba_best_val = model_probabilities[best_model_val_pred_key]\n",
    "\n",
    "    best_model_metrics = {\n",
    "        'accuracy': accuracy_score(y_val, y_pred_best_val),\n",
    "        'precision': precision_score(y_val, y_pred_best_val, zero_division=0),\n",
    "        'recall': recall_score(y_val, y_pred_best_val, zero_division=0),\n",
    "        'f1': f1_score(y_val, y_pred_best_val, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_val, y_proba_best_val) # This should match best_overall_roc_auc\n",
    "    }\n",
    "\n",
    "    best_model_info = {\n",
    "        'best_model_name': best_overall_model_name,\n",
    "        'saved_filename': model_filename,\n",
    "        'validation_metrics': best_model_metrics,\n",
    "        'parameters': best_model_instance.get_params() # Get all parameters of the best model\n",
    "    }\n",
    "\n",
    "    # Add tuning details if the best model was a tuned one and tuning info is available\n",
    "    if \"_tuned\" in best_overall_model_name:\n",
    "        gs_obj = None\n",
    "        original_model_type = best_overall_model_name.split('_')[0] # e.g. \"Logistic Regression\"\n",
    "\n",
    "        if \"Logistic Regression\" in original_model_type:\n",
    "            gs_obj = grid_search_log_reg\n",
    "        elif \"Random Forest\" in original_model_type:\n",
    "            gs_obj = grid_search_rf\n",
    "        elif \"LightGBM\" in original_model_type:\n",
    "            gs_obj = grid_search_lgbm\n",
    "\n",
    "        if gs_obj:\n",
    "            best_model_info['tuning_details'] = {\n",
    "                'best_cv_params': gs_obj.best_params_,\n",
    "                'best_cv_score': gs_obj.best_score_,\n",
    "                'cv_scoring_metric': SCORING_METRIC\n",
    "            }\n",
    "        else:\n",
    "            best_model_info['tuning_details'] = \"Tuning object not found for the best model.\"\n",
    "\n",
    "\n",
    "    best_model_summary_filename = \"../models/best_model_summary.json\"\n",
    "    try:\n",
    "        # Helper function to convert numpy types to native Python types for JSON serialization\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, np.bool_):\n",
    "                return bool(obj)\n",
    "            return obj # Or raise TypeError for unhandled types\n",
    "\n",
    "        with open(best_model_summary_filename, 'w') as f:\n",
    "            json.dump(best_model_info, f, indent=4, default=convert_numpy_types)\n",
    "        print(f\"Best model summary saved to {best_model_summary_filename}\")\n",
    "    except TypeError as e:\n",
    "        print(f\"Error serializing best model summary to JSON: {e}\")\n",
    "        print(\"Attempting to save with default=str as a fallback...\")\n",
    "        try:\n",
    "            with open(best_model_summary_filename, 'w') as f:\n",
    "                 json.dump(best_model_info, f, indent=4, default=str)\n",
    "            print(f\"Best model summary saved to {best_model_summary_filename} using default=str.\")\n",
    "        except Exception as e_fallback:\n",
    "            print(f\"Fallback saving also failed: {e_fallback}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while saving best model summary: {e}\")\n",
    "\n",
    "else:\n",
    "    if not best_overall_model_name:\n",
    "        print(\"Could not determine the best model. No validation scores found or scores were too low.\")\n",
    "    elif best_overall_model_name not in models:\n",
    "        print(f\"Best model '{best_overall_model_name}' determined by score, but not found in 'models' dictionary.\")\n",
    "\n",
    "print(\"\\n--- Stage 3 Completed ---\")"
   ],
   "id": "9c75b1f8497ae44d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall best model on validation set: LightGBM_tuned with ROC AUC: 0.7474\n",
      "Saved best model (LightGBM_tuned) to ../models/best_overall_model.joblib\n",
      "Best model summary saved to ../models/best_model_summary.json\n",
      "\n",
      "--- Stage 3 Completed ---\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
